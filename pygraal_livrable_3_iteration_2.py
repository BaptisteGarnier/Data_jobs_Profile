# -*- coding: utf-8 -*-
"""PyGraal_Livrable_3_ITERATION_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fPV9cjdaidkGh2o0EFzlaQ_xm6qfhTbT

# Introduction

Pour réaliser cette seconde itération, nous repartons du livrable précédent, à savoir:
- un dataset réduit aux 5 professions les plus représentées
- la variable cible définie sur 'Q5', la liste des professions
- train set (80%) et test set (20%)

De plus, pour la suite de l'analyse, nous ne gardons que les trois modèles les plus performants parmi les cinq testés, à savoir:

*   Logistique Régression 'lr'
*   Support Vector Machine 'svm'
*   RandomForest 'rf'
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_theme()

df_clean = pd.read_csv('/content/drive/MyDrive/PyGraal/df_clean.csv', index_col=0)

"""# Initialisation des modèles précédents"""

#Création du vecteur 'target' contenant la variable cible 'Q5' et d'un Data Frame 'feats' contenant les différentes features.
target = df_clean['Q5']
feats=df_clean.drop('Q5', axis=1)

#Séparation du dataset en train set et test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.2, random_state=200)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

"""## Modèle de Régression Logistique"""

lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)
lr_y_pred = lr.predict(X_test_scaled)

print('Score Train set de la Regression Logistique',round(lr.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de la Regression Logistique',round(lr.score(X_test_scaled, y_test)*100,2),'%')

lrCR = classification_report(y_test, lr_y_pred, output_dict=True)

lr1 = lr.score(X_train_scaled, y_train)
lr2 = lr.score(X_test_scaled, y_test)
lr3 = lrCR.get('macro avg', {}).get('precision')
lr4 = lrCR.get('macro avg', {}).get('recall')
lr5 = lrCR.get('macro avg', {}).get('f1-score')
lr6 = lrCR.get('weighted avg', {}).get('precision')
lr7 = lrCR.get('weighted avg', {}).get('recall')
lr8 = lrCR.get('weighted avg', {}).get('f1-score')

"""## Modèle SVM - Support Vector Machine"""

from sklearn import model_selection
from sklearn import svm

svm = svm.SVC()
svm.fit(X_train_scaled, y_train)

svm_y_pred = svm.predict(X_test_scaled)

print('Score Train set du modèle SVM', round(svm.score(X_train_scaled, y_train)*100,2), '%')
print('Score Test set du modèle SVM', round(svm.score(X_test_scaled, y_test)*100,2), '%')

svmCR = classification_report(y_test, svm_y_pred, output_dict=True)

svm1 = svm.score(X_train_scaled, y_train)
svm2 = svm.score(X_test_scaled, y_test)
svm3 = svmCR.get('macro avg', {}).get('precision')
svm4 = svmCR.get('macro avg', {}).get('recall')
svm5 = svmCR.get('macro avg', {}).get('f1-score')
svm6 = svmCR.get('weighted avg', {}).get('precision')
svm7 = svmCR.get('weighted avg', {}).get('recall')
svm8 = svmCR.get('weighted avg', {}).get('f1-score')

"""## Modèle Random Forest"""

rf = RandomForestClassifier ()
rf.fit(X_train_scaled, y_train)

rf_y_pred = rf.predict(X_test_scaled)

print('Score Train set du modèle Random Forest', round(rf.score(X_train_scaled, y_train)*100,2), '%')
print('Score Test set du modèle Random Forest', round(rf.score(X_test_scaled, y_test)*100,2), '%')

rfCR = classification_report(y_test, rf_y_pred, output_dict=True)

rf1 = rf.score(X_train_scaled, y_train)
rf2 = rf.score(X_test_scaled, y_test)
rf3 = rfCR.get('macro avg', {}).get('precision')
rf4 = rfCR.get('macro avg', {}).get('recall')
rf5 = rfCR.get('macro avg', {}).get('f1-score')
rf6 = rfCR.get('weighted avg', {}).get('precision')
rf7 = rfCR.get('weighted avg', {}).get('recall')
rf8 = rfCR.get('weighted avg', {}).get('f1-score')

"""## Comparaison des scores de chaque modèle entraîné"""

Data = {'Modele' : ['lr','svm','rf'],
        'Modele_Name' : ['LogisticRegression','SVM','RandomForestClassifier'],
        'Score_Train' : [lr1, svm1, rf1],
        'Score_Test' : [lr2, svm2, rf2],
        'precision(macro_avg)' : [lr3, svm3, rf3],
        'recall(macro_avg)' : [lr4, svm4, rf4],
        'f1_score(macro_avg)' : [lr5, svm5, rf5],
        'precision(weighted_avg)' : [lr6, svm6, rf6],
        'recall(weighted_avg)' : [lr7, svm7, rf7],
        'f1_score(weighted_avg)' : [lr8, svm8, rf8]}

Scores = pd.DataFrame(Data)

Scores.head()

plt.figure(figsize=(10,10))

plt.plot(Scores['Modele'],Scores['Score_Train'],color='red',marker='D',linewidth=2,label='Train')
plt.plot(Scores['Modele'],Scores['Score_Test'],color='blue',marker='D',linewidth=2,label='Test')
plt.xlabel('Modèle')
plt.ylabel('Scores')
plt.ylim([0.3,1])
plt.title('\nComparaison des scores des 3 modèles retenus\n\n',fontsize=20)
plt.legend(loc='upper center')

plt.show()

"""La visualisation des résultats avait permis de mettre en évidence un phénomène d'over-fitting ainsi que des performances à améliorer.

Le phènomène d'overfitting peut s'expliquer en partie par le déséquilibre de la variable cible, que l'on peut voir au travers de la répartition des 5 classes.
"""

plt.figure (figsize=(10,10))
plt.pie(df_clean['Q5'].value_counts(),
        autopct = lambda x: str(round(x, 2)) + '%',
        labels=df_clean['Q5'].value_counts().index,
        pctdistance=0.7,
        shadow =True,
        colors = ['#dddddd', '#81d8d0','#ffff66', '#ff7f50',
                  '#a0db8e'])
plt.title("Distribution du TOP 5 des métiers représentés",fontsize=15, fontweight='bold');

"""##Matrices de corrélation

Nous pouvons nous intéresser à la corrélation des variables, qui peut être source d'over-fitting, en cas de corrélation forte.
"""

#Encodage de la variable cible pour établir une heatmap
df_ML = df_clean
df_ML['Q5'] = df_ML['Q5'].replace(to_replace = ['Data Analyst','Data Scientist','Machine Learning Engineer','Research Scientist','Software Engineer'],
                                    value= [0,1,2,3,4])

plt.figure(figsize=(40,40))
sns.heatmap(df_ML.corr(), cmap = 'viridis', cbar = True)
plt.show()

"""La heatmap ne met pas en évidence des couleurs tranchées quant aux corrélations de la variable cible avec les features, regardons de plus près ses coefficents de corrélation."""

#Corrélations principales (positives et négatives) de la varaible cible avec les feats
df_ML.corr().iloc[0:,0:1].sort_values(by ='Q5',ascending=False)

#Analyse statistique rapide des corrélations de la variable cible
df_ML.corr()['Q5'].describe()

"""Les corrélations observées entre la variable cible (Q5: métiers de la data) et les variables features ne sont pas très élevées. 
Leur écart type est de 11,7%. De plus aucune variable n'est fortement corrélée positivement (maximum 28.5%) 
ou négativement (minimum -33.9%) à la variable cible.

**Nous allons chercher à corriger cela au travers de trois étapes:**

1.   **Réduction de dimensions**
2.  **Ré-échantillonnage**
1.   **Optimisation des hyperparamètres**

# Réduction de dimensions

Avant de tester des sélecteurs, il peut être intéressant de voir l'importance des variables au sein des modèles.
"""

#Feature_importances sur le Random Forest
feat_imp = rf.feature_importances_
Feats = list(feats.columns)
pd.DataFrame({'Feature importances': feat_imp}, index=Feats).sort_values(by = 'Feature importances', ascending=False).head(13)

"""Nous constatons que les importances sont faibles, 0.046 pour la plus importante."""

#Import de packages nécessaires à la feature selection
from sklearn.feature_selection import SelectKBest, SelectFromModel, f_classif, mutual_info_classif, RFECV
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold

"""## Utilisation de SelectKBest

### Paramètre score_func = mutual_info_classif
"""

sel = SelectKBest(score_func = mutual_info_classif)
sel.fit(X_train_scaled, y_train)
print('Les',len(feats.columns[sel.get_support()]),'features retenues sont :\n', feats.columns[sel.get_support()])

#Transformation des train set et test set
sel_train = sel.transform(X_train_scaled)
sel_test = sel.transform(X_test_scaled)

lr.fit(sel_train, y_train)

print("Score du train set:", round(lr.score(sel_train,y_train)*100,2),'%')
print("Score du test set:", round(lr.score(sel_test,y_test)*100,2),'%')

svm.fit(sel_train, y_train)

print("Score du train set:", round(svm.score(sel_train,y_train)*100,2),'%')
print("Score du test set:", round(svm.score(sel_test,y_test)*100,2),'%')

rf.fit(sel_train, y_train)

print("Score du train set:", round(rf.score(sel_train,y_train)*100,2),'%')
print("Score du test set:", round(rf.score(sel_test,y_test)*100,2),'%')

"""### Paramètre score_func = f_classif"""

sel2 = SelectKBest(score_func = f_classif)
sel2.fit(X_train_scaled, y_train)
print('Les',len(feats.columns[sel2.get_support()]),'features retenues sont :\n', feats.columns[sel2.get_support()])

#Transformation des train set et test set
sel2_train = sel2.transform(X_train_scaled)
sel2_test = sel2.transform(X_test_scaled)

lr.fit(sel2_train, y_train)

print("Score du train set:", round(lr.score(sel2_train,y_train)*100,2),'%')
print("Score du test set:", round(lr.score(sel2_test,y_test)*100,2),'%')

svm.fit(sel2_train, y_train)

print("Score du train set:", round(svm.score(sel2_train,y_train)*100,2),'%')
print("Score du test set:", round(svm.score(sel2_test,y_test)*100,2),'%')

rf.fit(sel2_train, y_train)

print("Score du train set:", round(rf.score(sel2_train,y_train)*100,2),'%')
print("Score du test set:", round(rf.score(sel2_test,y_test)*100,2),'%')

"""## Utilisation de SelectFromModel"""

sfm_lr = SelectFromModel(lr, threshold='mean')
sfm_lr.fit(X_train_scaled,y_train)

print("Nombre de features retenues:", len(sfm_lr.get_support()[sfm_lr.get_support()==True]))

sfm_lr_train =sfm_lr.transform(X_train_scaled)
sfm_lr_test = sfm_lr.transform(X_test_scaled)

lr.fit(sfm_lr_train, y_train)

print("Score du train set:", round(lr.score(sfm_lr_train,y_train)*100,2),'%')
print("Score du test set:", round(lr.score(sfm_lr_test,y_test)*100,2),'%')

svm = SVC(kernel='linear')
sfm_svm = SelectFromModel(svm, threshold='mean')
sfm_svm.fit(X_train_scaled,y_train)

print("Nombre de features retenues:", len(sfm_svm.get_support()[sfm_svm.get_support()==True]))

sfm_svm_train =sfm_svm.transform(X_train_scaled)
sfm_svm_test = sfm_svm.transform(X_test_scaled)



svm.fit(sfm_svm_train, y_train)

print("Score du train set:", round(svm.score(sfm_svm_train,y_train)*100,2),'%')
print("Score du test set:", round(svm.score(sfm_svm_test,y_test)*100,2),'%')

sfm_rf = SelectFromModel(rf, threshold='mean')
sfm_rf.fit(X_train_scaled,y_train)

print("Nombre de features retenues:", len(sfm_rf.get_support()[sfm_rf.get_support()==True]))

sfm_rf_train =sfm_rf.transform(X_train_scaled)
sfm_rf_test = sfm_rf.transform(X_test_scaled)

rf.fit(sfm_rf_train, y_train)

print("Score du train set:", round(rf.score(sfm_rf_train,y_train)*100,2),'%')
print("Score du test set:", round(rf.score(sfm_rf_test,y_test)*100,2),'%')

"""## Utilisation de RFECV"""

lr = LogisticRegression()

rfecv_lr = RFECV(estimator=lr, cv = 5, step=1)
rfecv_lr.fit(X_train_scaled, y_train)

plt.plot(rfecv_lr.grid_scores_);
print("\nLa rfecv de la Regression Logistique retient",rfecv_lr.n_features_," features.")

print("Le score moyen pour les",rfecv_lr.n_features_,"features retenues est de",round(rfecv_lr.grid_scores_[rfecv_lr.n_features_].mean()*100,2),'%')

"""## Utilisation de l'ACP"""

#Initialisation de l'ACP avec la variable cible encodée
target2 = df_ML['Q5']
feats2=df_ML.drop('Q5', axis=1) 

X_train2, X_test2, y_train2, y_test2 = train_test_split(feats2, target2, test_size=0.2, random_state=200)

sc = StandardScaler()
X_train_scaled2 = sc.fit_transform(X_train2)
X_test_scaled2 = sc.transform(X_test2)

from sklearn.decomposition import PCA                #Transformeur : Analyse en Composantes Principales

pca = PCA(n_components = 2)
coord = pca.fit_transform(feats2)

fig = plt.figure()

ax = fig.add_subplot(111)
ax.scatter(coord[:, 0], coord[:, 1], c = target2, cmap = plt.cm.Spectral)

ax.set_xlabel('PCA 1')
ax.set_ylabel('PCA 2')

ax.set_title("Donnees projetees sur les 2 axes PCA")
plt.show();

print("La part de variance expliquee est :", round(pca.explained_variance_ratio_.sum(), 2))

pca = PCA()
pca.fit(feats)

plt.figure()
plt.xlim(0,100)
plt.plot(pca.explained_variance_ratio_);

plt.figure()
plt.xlim(0,100)
plt.axhline(y = 0.9, color ='r', linestyle = '--')
plt.plot(pca.explained_variance_ratio_.cumsum());

"""On a pour habitude de considérer que conserver 90% de la variance d'un jeu de données est un seuil acceptable pour la PCA. Sur le deuxième graphe, on peut lire qu'il faut environ 70 Composantes Principales pour obtenir 90% de variance expliquée."""

pca = PCA(n_components = 0.9)
coord = pca.fit_transform(feats2)

fig = plt.figure()

ax = fig.add_subplot(111)
ax.scatter(coord[:, 0], coord[:, 1], c = target2, cmap = plt.cm.Spectral)

ax.set_xlabel('PCA 1')
ax.set_ylabel('PCA 2')

ax.set_title("Donnees projetees sur les 2 axes PCA")
plt.show();

print("La part de variance expliquee est :", round(pca.explained_variance_ratio_.sum(), 2))

pca = PCA(n_components = 0.9)
X_train_PCA = pca.fit_transform(X_train)
X_test_PCA = pca.transform(X_test)

lr = LogisticRegression()
lr.fit(X_train_PCA, y_train)
print("Score du train set:", round(lr.score(X_train_PCA,y_train)*100,2),'%')
print("Score du test set:", round(lr.score(X_test_PCA,y_test)*100,2),'%')

svm = SVC()
svm.fit(X_train_PCA, y_train)
print("Score du train set:", round(svm.score(X_train_PCA,y_train)*100,2),'%')
print("Score du test set:", round(svm.score(X_test_PCA,y_test)*100,2),'%')

rf = RandomForestClassifier()
rf.fit(X_train_PCA, y_train)
print("Score du train set:", round(rf.score(X_train_PCA,y_train)*100,2),'%')
print("Score du test set:", round(rf.score(X_test_PCA,y_test)*100,2),'%')

"""Ces divers tests n'ont pas permis d'améliorer significativement les scores test et train initialement observés pour chacun des modèles.

Le modèle de Régression Logistique semble se distinguer.

# Ré-échantillonnage
"""

#Import de packages nécessaires au ré-échantillonange
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler,  ClusterCentroids

"""## Utilisation de RandomUnderSampler"""

(X_ru, y_ru) = RandomUnderSampler().fit_resample(X_train_scaled, y_train)
print('Classes échantillon undersampled :', dict(pd.Series(y_ru).value_counts()))

lr_u = LogisticRegression()

lr_u.fit(X_ru, y_ru)
y_pred_lr_u= lr_u.predict(X_test_scaled)

print('Train Score lr_u :', round(lr_u.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score lr_u :', round(lr_u.score(X_test_scaled, y_test)*100,2),'%')

# Support Vector Machines - SVM Undersampling
svm_ru = SVC()

svm_ru.fit(X_ru, y_ru)
y_pred_svm_u= svm_ru.predict(X_test_scaled)

print('Score Train set de SVM RandomUnderSampling',round(svm_ru.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de SVM RandomUnderSampling',round(svm_ru.score(X_test_scaled, y_test)*100,2),'%')

# Random Forest - RF Undersampling
rf_ru = RandomForestClassifier()

rf_ru.fit(X_ru, y_ru)
y_pred_rf_u= rf_ru.predict(X_test_scaled)

print('Score Train set de Random Forest RandomUnderSampling',round(rf_ru.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de Random Forest RandomUnderSampling',round(rf_ru.score(X_test_scaled, y_test)*100,2),'%')

"""## Utilisation de ClusterCentroids"""

(X_cc, y_cc) = ClusterCentroids().fit_resample(X_train_scaled, y_train)
print('Classes échantillon ClusterCentroids :', dict(pd.Series(y_cc).value_counts()))

lr_cc = LogisticRegression()

lr_cc.fit(X_cc, y_cc)
y_pred_lr_cc= lr_cc.predict(X_test_scaled)

print('Train Score lr_cc :', round(lr_cc.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score lr_cc :', round(lr_cc.score(X_test_scaled, y_test)*100,2),'%')

# Support Vector Machine - SVM Centroids
svm_cc = SVC()

svm_cc.fit(X_cc, y_cc)
y_pred_svm_cc= svm_cc.predict(X_test_scaled)

print('Score Train set de SVM ClusterCentroids',round(svm_cc.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de SVM ClusterCentroids',round(svm_cc.score(X_test_scaled, y_test)*100,2),'%')

# Random Forest - RF Centroids
rf_cc = RandomForestClassifier()

rf_cc.fit(X_cc, y_cc)
y_pred_rf_cc= rf_cc.predict(X_test_scaled)

print('Score Train set de Random Forest ClusterCentroids',round(rf_cc.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de Random Forest ClusterCentroids',round(rf_cc.score(X_test_scaled, y_test)*100,2),'%')

"""## Utilisation de class_weight

### Class_weight = dictionnaire manuel
"""

#A partir de la répartition des cinq classes de la variable cible, nous pouvons ré-équilibrer les classes.
lr_w = LogisticRegression(class_weight={'Data Scientist':0.10,
                                        'Software Engineer' : 0.15,
                                        'Data Analyst': 0.25,
                                        'Research Scientist' : 0.25,
                                        'Machine Learning Engineer' : 0.25})
lr_w.fit(X_train_scaled, y_train)                         

y_pred_w = lr_w.predict(X_test_scaled)

print('Train Score lr_w :', round(lr_w.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score lr_w :', round(lr_w.score(X_test_scaled, y_test)*100,2),'%')

svm_w = SVC(class_weight={'Data Scientist':0.10,
                                        'Software Engineer' : 0.15,
                                        'Data Analyst': 0.25,
                                        'Research Scientist' : 0.25,
                                        'Machine Learning Engineer' : 0.25})
svm_w.fit(X_train_scaled, y_train)

print('Train Score SVM_W :', round(svm_w.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score SVM_W :', round(svm_w.score(X_test_scaled, y_test)*100,2),'%')

rf_w = RandomForestClassifier(class_weight={'Data Scientist':0.10,
                                        'Software Engineer' : 0.15,
                                        'Data Analyst': 0.25,
                                        'Research Scientist' : 0.25,
                                        'Machine Learning Engineer' : 0.25})
rf_w.fit(X_train_scaled, y_train)

print('Train Score rf_W :', round(rf_w.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score rf_W :', round(rf_w.score(X_test_scaled, y_test)*100,2),'%')

"""### Class_weight = 'balanced'"""

svm_w2 = SVC(class_weight='balanced')
svm_w2.fit(X_train_scaled, y_train)

y_pred_svm_w2 = svm_w2.predict(X_test_scaled)

print('Train Score SVM_W2 :', round(svm_w2.score(X_train_scaled, y_train)*100,2),'%')
print('Test Score SVM_W2 :', round(svm_w2.score(X_test_scaled, y_test)*100,2),'%')

"""## Utilisation de RandomOverSampling"""

rOs = RandomOverSampler()
X_ro, y_ro = rOs.fit_resample(X_train_scaled, y_train)
print('Classes échantillon oversampled :', dict(pd.Series(y_ro).value_counts()))

# Support Vector Machines - SVM Oversampling
svm_ro = SVC()

svm_ro.fit(X_ro, y_ro)
y_pred_o= svm_ro.predict(X_test_scaled)

print('Score Train set de SVM RandomOverSampling',round(svm_ro.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de SVM RandomOverSampling',round(svm_ro.score(X_test_scaled, y_test)*100,2),'%')

"""## Utilisation de SMOTE"""

smo = SMOTE()
X_sm, y_sm = smo.fit_resample(X_train_scaled, y_train)
print('Classes échantillon SMOTE :', dict(pd.Series(y_sm).value_counts()))

# Support Vector Machines - SVM SMOTE
svm_sm = SVC()

svm_sm.fit(X_sm, y_sm)
y_pred_sm= svm_sm.predict(X_test_scaled)

print('Score Train set de SVM SMOTE',round(svm_sm.score(X_train_scaled, y_train)*100,2),'%')
print('Score Test set de SVM SMOTE',round(svm_sm.score(X_test_scaled, y_test)*100,2),'%')

"""# Optimisation des hyperparamètres

Suite à ces différents tests, nous optons pour chaque modèle à l'élaboration d'un pipeline avec:


*   Classifieur ré-équilibré manuellement
*   RFECV propre à chaque classifieur sur 5 CV
*   GridSearch sur 5 CV
"""

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

"""## Optimisation du modèle de Régression Logistique"""

lr = LogisticRegression(solver ='liblinear',
                             random_state=42,
                             class_weight={'Data Scientist':0.10,
                                        'Software Engineer' : 0.15,
                                        'Data Analyst': 0.25,
                                        'Research Scientist' : 0.25,
                                        'Machine Learning Engineer' : 0.25})

rfecv_lr = RFECV(estimator=lr, cv = 5, step=1)

param_grid_lr = {'solver': ['liblinear', 'lbfgs'],'C': [10**(i) for i in range(-4, 3)]}


GS_lr = GridSearchCV(lr, 
                      param_grid=param_grid_lr,
                      cv= 5)

pipeline_lr  = Pipeline([('feature_sele',rfecv_lr),
                      ('lr_cv',GS_lr)])

pipeline_lr.fit(X_train_scaled, y_train)
pipeline_lr.predict(X_test_scaled)

print("Train score du Pipeline_lr :",round(pipeline_lr.score(X_train_scaled, y_train)*100,2))
print("Test score du Pipeline_lr :",round(pipeline_lr.score(X_test_scaled, y_test)*100,2))

print("Nombre de features retenus pour la Logistique Régression :", rfecv_lr.n_features_)
print("Meilleurs paramètres retenus pour la Logistique Régression :",GS_lr.best_params_)

"""## Optimisation du modèle de Support Vector Machine

Malgré de nombreuses tentatives, nous n'avons pas réussi à exécuter le code suivant du pipeline sur ce modèle.
"""

from sklearn.pipeline import Pipeline
from sklearn.svm import SVC                          #Modèle : Support vector Classifier
from sklearn.decomposition import PCA                #Transformeur : Analyse en Composantes Principales

pca = PCA()
svm = SVC()

param_grid = {'kernel' : ['poly', 'linear', 'rbf'],
              'C' : [0.1, 1, 10, 100],
              'gamma' : [0.001, 0.1, 1, 10]}

grid_svm = GridSearchCV(svm, param_grid = param_grid, cv= 5)

composite_model = Pipeline(steps = [('pca', pca),
                                    ('model', grid_svm)])

"""composite_model.fit(X_train_scaled, y_train)
composite_model.predict(X_test_scaled)
composite_model.score(X_test_scaled, y_test)

Toutefois, nous tentons une optimisation à l'aide du sélecteur 'SelectFromModel' et d'un kernel 'linear'
"""

svm = SVC(kernel='linear')
sfm_svm = SelectFromModel(svm, threshold='mean')
sfm_svm.fit(X_train_scaled,y_train)

print("Nombre de features retenues:", len(sfm_svm.get_support()[sfm_svm.get_support()==True]))

sfm_svm_train =sfm_svm.transform(X_train_scaled)
sfm_svm_test = sfm_svm.transform(X_test_scaled)

svm.fit(sfm_svm_train, y_train)

print("Score du train set:", round(svm.score(sfm_svm_train,y_train)*100,2),'%')
print("Score du test set:", round(svm.score(sfm_svm_test,y_test)*100,2),'%')

"""## Optimisation du modèle de Random Forest

Pour ce modèle, les tests s'exécutent entre 15min et 30min, en fonction des cv {3,10} / class_weight.

Après regroupement sur ce notebook, ce modèle n'a pas ressorti les mêmes résultats, à savoir:

score train 0.8137 / score test 0.5391
"""

crossval = KFold(n_splits = 5, random_state = 2, shuffle = True)
rfecv_rf = RFECV(estimator=rf, cv = crossval, step=1)

param_grid_rf = [{'n_estimators': [10, 50, 100, 250, 500, 1000],
                    'min_samples_leaf': [1, 3, 5],
                    'max_features': ['sqrt', 'log2']}]


GS_rf = GridSearchCV(rf, 
                      param_grid=param_grid_rf,
                      cv= crossval)

#rfecv_rf : application d'un RFECV avec Crossval
pipeline_rf  = Pipeline([('feature_sele',rfecv_rf),
                      ('rf_cv',GS_rf)])

pipeline_rf.fit(X_train_scaled, y_train)
pipeline_rf.predict(X_test_scaled)

print("Train score du Pipeline_rf :",round(pipeline_rf.score(X_train_scaled, y_train)*100,2),'%')
print("Test score du Pipeline_rf :",round(pipeline_rf.score(X_test_scaled, y_test)*100,2),'%')

print("Nombre de features retenues pour le modèle Random Forest :", rfecv_rf.n_features_)
print("Meilleurs paramètres retenus pour le modèle Random Forest :", GS_rf.best_params_)

"""## Comparaison des performances des modèles optimisés"""

lr10 = pipeline_lr.score(X_train_scaled, y_train)
lr20 = pipeline_lr.score(X_test_scaled, y_test)
svm10 = svm.score(sfm_svm_train,y_train)
svm20 = svm.score(sfm_svm_test,y_test)
rf10 = 0.8137     #pipeline_rf.score(X_train_scaled, y_train)
rf20 = 0.5391     #pipeline_rf.score(X_test_scaled, y_test)

Data = {'Modele' : ['lr','svm','rf'],
        'Modele_Name' : ['LogisticRegression','SVM','RandomForestClassifier'],
        'Score_Train' : [lr1, svm1, rf1],
        'Score_Test' : [lr2, svm2, rf2],
        'Score_Train_opt' : [lr10, svm10, rf10],
        'Score_Test_opt' : [lr20, svm20, rf20]}

Scores2 = pd.DataFrame(Data)

Scores2.head()

plt.figure(figsize=(10,10))

plt.plot(Scores2['Modele'],Scores2['Score_Train'],color='red',marker='D',linewidth=2,label='Train')
plt.plot(Scores2['Modele'],Scores2['Score_Test'],color='blue',marker='D',linewidth=2,label='Test')
plt.plot(Scores2['Modele'],Scores2['Score_Train_opt'],color='red',ls=':',marker='D',linewidth=2,label='Train_opt')
plt.plot(Scores2['Modele'],Scores2['Score_Test_opt'],color='blue',ls=':',marker='D',linewidth=2,label='Test_opt')

plt.xlabel('Modèle')
plt.ylabel('Scores')
plt.ylim([0.3,1])
plt.title('\nComparaison des scores des 3 modèles retenus\n\n',fontsize=20)
plt.legend(loc='upper center')

plt.show()

"""# Conclusion et perspectives

Notre étude des différents métiers de la data nous a conduit à analyser un jeu de données représentant l’ensemble des réponses de plus de 20 000 individus à une quarantaine de questions, ceci dans le but d’élaborer un modèle de prédiction de postes data en fonction des compétences des individus.

Après exclusion des non-professionnels (étudiants et sans-emploi) , le panel de professions classifiées a été réduit de 10 à 5 (soit 80% du panel de répondants) afin de ne pas surcharger la modélisation.

Cinq modèles ont été retenus : Logistic Regression, SVM, Random Forest, Decision Tree et KNN , produisant des prédictions dont la précision (score test) variait entre 38 et 54%.

Une faible corrélation des variables entre elles ainsi qu’un surapprentissage des modèles nous ont conduit à explorer différentes techniques (réduction de dimensions, rééchantillonnage, optimisation des hyperparamètres) dans le but d’améliorer la performance globale des modèles.

Cependant nos différents tests ne nous ont pas permis d’observer une  amélioration notable des scores aussi bien dans le surapprentissage que dans la précision globale.


Plusieurs hypothèses peuvent rentrer en ligne de compte pour expliquer ces scores :

•	Le domaine de la Data Science est encore récent et en évolution, ne 
permettant pas une distinction nette de ses postes au vu des compétences et tâches quotidiennes. 

•	Des besoins de polyvalence et d’interchangeabilité peuvent aussi être recherchés par les entreprises, soit dans un but évolutif ou tout simplement car la taille de l’entreprise ne permet pas de recruter des spécialistes de chaque métier.

•	On ne peut aussi exclure que le questionnaire, bien que fourni, puisse avoir été enrichi de questions complémentaires qui auraient accentuées les distinctions potentielles entre chaque métier.



Toutefois, nous retenons le modèle **Support Vector Machine** qui présente les meilleures performances relatives, avec pour paramètre kernel = 'linear' et une réduction de dimensions via SelectFromModel.
"""